# 1.4 Claude Code Integration

**Deliverable**: Claude Code SDK integration, rate limiting, and conversation management

## Overview

The Claude Code integration layer provides sophisticated management of headless SDK sessions with intelligent rate limiting, context optimization, and robust error recovery. This component is critical for maintaining efficient and cost-effective operation while maximizing task throughput.

## Headless SDK Architecture

### Session Management

The system maintains persistent Claude Code sessions with intelligent lifecycle management:

```rust
use claude_code_sdk::{HeadlessClient, SessionConfig, Message, ToolUse, ContentBlock};

#[derive(Debug)]
pub struct ClaudeCodeInterface {
    client: Arc<HeadlessClient>,
    session_pool: Arc<Mutex<SessionPool>>,
    rate_limiter: Arc<RateLimiter>,
    context_manager: Arc<ContextManager>,
    usage_tracker: Arc<UsageTracker>,
    error_recovery: Arc<ErrorRecoveryManager>,
}

#[derive(Debug, Clone)]
pub struct SessionPool {
    active_sessions: HashMap<SessionId, ClaudeSession>,
    idle_sessions: VecDeque<ClaudeSession>,
    max_concurrent_sessions: u32,
    session_timeout: Duration,
    context_window_size: u32,
}

#[derive(Debug, Clone)]
pub struct ClaudeSession {
    pub id: SessionId,
    pub created_at: DateTime<Utc>,
    pub last_used: DateTime<Utc>,
    pub message_count: u32,
    pub token_usage: TokenUsage,
    pub context_summary: ContextSummary,
    pub model_config: ModelConfiguration,
    pub conversation_state: ConversationState,
}

impl ClaudeCodeInterface {
    pub async fn new(config: ClaudeConfig) -> Result<Self> {
        let client = HeadlessClient::new(config.api_key, config.endpoint).await?;

        let session_pool = Arc::new(Mutex::new(SessionPool::new(config.session_config)));
        let rate_limiter = Arc::new(RateLimiter::new(config.rate_limits));
        let context_manager = Arc::new(ContextManager::new(config.context_config));
        let usage_tracker = Arc::new(UsageTracker::new(config.usage_tracking));
        let error_recovery = Arc::new(ErrorRecoveryManager::new(config.error_config));

        Ok(ClaudeCodeInterface {
            client,
            session_pool,
            rate_limiter,
            context_manager,
            usage_tracker,
            error_recovery,
        })
    }

    pub async fn execute_task_request(
        &self,
        request: TaskRequest,
        context: TaskContext,
    ) -> Result<TaskResponse> {
        // Select or create appropriate session
        let session = self.get_or_create_session(&request, &context).await?;

        // Apply rate limiting
        self.rate_limiter.acquire_permit(&request).await?;

        // Execute request with error recovery
        let response = self.execute_with_recovery(&session, request, context).await?;

        // Update usage tracking
        self.usage_tracker.record_usage(&session, &response).await?;

        // Update session state
        self.update_session_state(&session, &response).await?;

        Ok(response)
    }
}
```

### Context Optimization

The system implements intelligent context window management to maximize efficiency:

```rust
#[derive(Debug, Clone)]
pub struct ContextManager {
    window_size: u32,
    compression_threshold: f64,
    relevance_scoring: RelevanceScorer,
    context_cache: Arc<Mutex<ContextCache>>,
}

#[derive(Debug, Clone)]
pub struct ContextWindow {
    pub messages: Vec<Message>,
    pub token_count: u32,
    pub relevance_scores: HashMap<usize, f64>,
    pub compression_ratio: f64,
    pub last_optimized: DateTime<Utc>,
}

impl ContextManager {
    pub async fn optimize_context_for_task(
        &self,
        current_context: &ConversationState,
        task: &Task,
    ) -> Result<OptimizedContext> {
        // Score message relevance for the current task
        let relevance_scores = self.score_message_relevance(current_context, task).await?;

        // Identify messages for compression or removal
        let (keep_messages, compress_messages, remove_messages) =
            self.categorize_messages(&relevance_scores).await?;

        // Apply compression strategies
        let compressed_context = self.apply_compression_strategies(
            &keep_messages,
            &compress_messages,
            task,
        ).await?;

        // Validate context window constraints
        let optimized = self.validate_and_adjust_context(compressed_context).await?;

        Ok(optimized)
    }

    async fn score_message_relevance(
        &self,
        context: &ConversationState,
        task: &Task,
    ) -> Result<HashMap<MessageId, f64>> {
        let mut scores = HashMap::new();

        for message in &context.messages {
            let relevance_score = self.calculate_relevance_score(message, task).await?;
            scores.insert(message.id, relevance_score);
        }

        Ok(scores)
    }

    async fn calculate_relevance_score(&self, message: &Message, task: &Task) -> Result<f64> {
        let mut score = 0.0;

        // Temporal relevance (recent messages scored higher)
        let time_factor = self.calculate_temporal_relevance(&message.timestamp);
        score += time_factor * 0.3;

        // Content relevance (keyword and semantic matching)
        let content_factor = self.calculate_content_relevance(message, task).await?;
        score += content_factor * 0.4;

        // Context relevance (related to current task context)
        let context_factor = self.calculate_context_relevance(message, task).await?;
        score += context_factor * 0.3;

        Ok(score.min(1.0))
    }

    async fn apply_compression_strategies(
        &self,
        keep_messages: &[Message],
        compress_messages: &[Message],
        task: &Task,
    ) -> Result<Vec<Message>> {
        let mut result = keep_messages.to_vec();

        // Apply different compression strategies
        for message in compress_messages {
            let compressed = match &message.content {
                ContentBlock::Text(text) => {
                    self.compress_text_content(text, task).await?
                }
                ContentBlock::ToolUse(tool_use) => {
                    self.compress_tool_use(tool_use).await?
                }
                ContentBlock::ToolResult(tool_result) => {
                    self.compress_tool_result(tool_result).await?
                }
            };

            result.push(Message {
                id: message.id,
                role: message.role.clone(),
                content: compressed,
                timestamp: message.timestamp,
                metadata: message.metadata.clone(),
            });
        }

        Ok(result)
    }
}
```

## Rate Limiting System

### Adaptive Rate Limiting

The system implements sophisticated rate limiting based on patterns from the ccusage project:

```rust
#[derive(Debug, Clone)]
pub struct RateLimiter {
    token_bucket: Arc<Mutex<TokenBucket>>,
    usage_history: Arc<Mutex<UsageHistory>>,
    backoff_strategy: BackoffStrategy,
    usage_predictor: Arc<UsagePredictor>,
}

#[derive(Debug, Clone)]
pub struct TokenBucket {
    current_tokens: u64,
    max_tokens: u64,
    refill_rate: u64,      // tokens per minute
    last_refill: DateTime<Utc>,
    request_count: u64,
    max_requests_per_minute: u64,
}

#[derive(Debug, Clone)]
pub struct UsageHistory {
    sessions: VecDeque<UsageSession>,
    current_session: UsageSession,
    retention_duration: Duration,
}

#[derive(Debug, Clone)]
pub struct UsageSession {
    pub start_time: DateTime<Utc>,
    pub end_time: Option<DateTime<Utc>>,
    pub token_usage: TokenUsage,
    pub request_count: u64,
    pub error_count: u64,
    pub average_response_time: Duration,
}

impl RateLimiter {
    pub async fn acquire_permit(&self, request: &TaskRequest) -> Result<RatePermit> {
        let estimated_tokens = self.estimate_token_usage(request).await?;

        // Check if we can proceed immediately
        if self.can_proceed_immediately(estimated_tokens).await? {
            return self.grant_permit(estimated_tokens).await;
        }

        // Calculate required wait time
        let wait_time = self.calculate_wait_time(estimated_tokens).await?;

        // Apply adaptive backoff if needed
        let adjusted_wait = self.apply_adaptive_backoff(wait_time).await?;

        // Wait for permit availability
        self.wait_for_permit(adjusted_wait).await?;

        self.grant_permit(estimated_tokens).await
    }

    async fn calculate_wait_time(&self, required_tokens: u64) -> Result<Duration> {
        let bucket = self.token_bucket.lock().await;

        if required_tokens <= bucket.current_tokens {
            return Ok(Duration::ZERO);
        }

        let tokens_needed = required_tokens - bucket.current_tokens;
        let time_to_refill = Duration::from_secs(
            (tokens_needed * 60) / bucket.refill_rate
        );

        // Consider request rate limits as well
        let request_wait = self.calculate_request_rate_wait().await?;

        Ok(time_to_refill.max(request_wait))
    }

    async fn apply_adaptive_backoff(&self, base_wait: Duration) -> Result<Duration> {
        let history = self.usage_history.lock().await;
        let recent_errors = self.count_recent_errors(&history, Duration::from_minutes(10));

        if recent_errors == 0 {
            return Ok(base_wait);
        }

        // Exponential backoff with jitter
        let backoff_multiplier = 2.0_f64.powi(recent_errors.min(5) as i32);
        let jitter = rand::random::<f64>() * 0.1 + 0.95; // 95-105% jitter

        let adjusted_wait = Duration::from_millis(
            (base_wait.as_millis() as f64 * backoff_multiplier * jitter) as u64
        );

        Ok(adjusted_wait.min(Duration::from_minutes(10))) // Cap at 10 minutes
    }
}
```

### Usage Prediction and Optimization

The system predicts future usage patterns to optimize rate limiting:

```rust
#[derive(Debug)]
pub struct UsagePredictor {
    historical_data: Arc<Mutex<Vec<UsageDataPoint>>>,
    model: PredictionModel,
    learning_rate: f64,
}

#[derive(Debug, Clone)]
pub struct UsageDataPoint {
    pub timestamp: DateTime<Utc>,
    pub task_type: String,
    pub token_usage: u64,
    pub response_time: Duration,
    pub success: bool,
}

impl UsagePredictor {
    pub async fn predict_usage_pattern(
        &self,
        task_queue: &[Task],
        time_horizon: Duration,
    ) -> Result<UsagePrediction> {
        let historical = self.historical_data.lock().await;

        // Analyze task patterns
        let task_patterns = self.analyze_task_patterns(&historical, task_queue).await?;

        // Predict token usage over time
        let token_prediction = self.predict_token_usage(&task_patterns, time_horizon).await?;

        // Identify potential bottlenecks
        let bottlenecks = self.identify_bottlenecks(&token_prediction).await?;

        // Generate optimization suggestions
        let optimizations = self.suggest_optimizations(&bottlenecks, task_queue).await?;

        Ok(UsagePrediction {
            time_horizon,
            predicted_token_usage: token_prediction,
            estimated_cost: self.calculate_estimated_cost(&token_prediction).await?,
            bottlenecks,
            optimization_suggestions: optimizations,
            confidence_level: self.calculate_confidence(&historical).await?,
        })
    }

    async fn suggest_optimizations(
        &self,
        bottlenecks: &[Bottleneck],
        task_queue: &[Task],
    ) -> Result<Vec<OptimizationSuggestion>> {
        let mut suggestions = Vec::new();

        for bottleneck in bottlenecks {
            match bottleneck.bottleneck_type {
                BottleneckType::TokenRateLimit => {
                    suggestions.push(OptimizationSuggestion::BatchSimilarTasks {
                        affected_tasks: bottleneck.affected_tasks.clone(),
                        estimated_savings: bottleneck.impact.token_savings,
                    });

                    suggestions.push(OptimizationSuggestion::ContextOptimization {
                        sessions_to_optimize: bottleneck.affected_sessions.clone(),
                        compression_opportunity: bottleneck.impact.context_reduction,
                    });
                }

                BottleneckType::RequestRateLimit => {
                    suggestions.push(OptimizationSuggestion::RequestSpacing {
                        optimal_interval: bottleneck.suggested_interval,
                        affected_period: bottleneck.time_window,
                    });
                }

                BottleneckType::CostLimit => {
                    suggestions.push(OptimizationSuggestion::CostOptimization {
                        model_downgrade_candidates: self.find_downgrade_candidates(task_queue).await?,
                        task_prioritization: self.suggest_task_prioritization(task_queue).await?,
                    });
                }
            }
        }

        Ok(suggestions)
    }
}
```

## Error Recovery and Resilience

### Comprehensive Error Handling

The integration implements robust error recovery mechanisms:

```rust
#[derive(Debug)]
pub struct ErrorRecoveryManager {
    retry_strategies: HashMap<ErrorType, RetryStrategy>,
    circuit_breaker: Arc<CircuitBreaker>,
    error_analytics: Arc<ErrorAnalytics>,
}

#[derive(Debug, Clone)]
pub enum ErrorType {
    RateLimit,
    NetworkTimeout,
    ServiceUnavailable,
    AuthenticationFailure,
    ModelOverloaded,
    ContextTooLarge,
    InvalidRequest,
    UnknownError,
}

#[derive(Debug, Clone)]
pub struct RetryStrategy {
    pub max_attempts: u32,
    pub base_delay: Duration,
    pub max_delay: Duration,
    pub backoff_multiplier: f64,
    pub jitter: bool,
    pub should_retry: Box<dyn Fn(&ClaudeError) -> bool + Send + Sync>,
}

impl ErrorRecoveryManager {
    pub async fn execute_with_recovery<F, T>(
        &self,
        session: &ClaudeSession,
        operation: F,
    ) -> Result<T>
    where
        F: Fn() -> BoxFuture<'static, Result<T>> + Send + Sync,
        T: Send + Sync,
    {
        let mut attempt = 0;
        let mut last_error = None;

        while attempt < self.get_max_attempts().await {
            // Check circuit breaker
            if !self.circuit_breaker.can_proceed().await {
                return Err(ClaudeError::CircuitBreakerOpen);
            }

            match operation().await {
                Ok(result) => {
                    self.circuit_breaker.record_success().await;
                    return Ok(result);
                }
                Err(error) => {
                    attempt += 1;
                    last_error = Some(error.clone());

                    // Record error for analytics
                    self.error_analytics.record_error(&error, session).await?;

                    // Check if we should retry
                    if !self.should_retry(&error).await {
                        break;
                    }

                    // Apply recovery strategy
                    if let Some(recovery_action) = self.get_recovery_action(&error).await {
                        self.apply_recovery_action(recovery_action, session).await?;
                    }

                    // Calculate delay before retry
                    let delay = self.calculate_retry_delay(attempt, &error).await;
                    tokio::time::sleep(delay).await;

                    self.circuit_breaker.record_failure().await;
                }
            }
        }

        Err(last_error.unwrap_or(ClaudeError::MaxRetriesExceeded))
    }

    async fn get_recovery_action(&self, error: &ClaudeError) -> Option<RecoveryAction> {
        match error {
            ClaudeError::ContextTooLarge => {
                Some(RecoveryAction::CompressContext {
                    compression_ratio: 0.7,
                    preserve_recent: true,
                })
            }

            ClaudeError::RateLimit { reset_time } => {
                Some(RecoveryAction::WaitUntil {
                    wait_until: *reset_time,
                    fallback_delay: Duration::from_secs(60),
                })
            }

            ClaudeError::ModelOverloaded => {
                Some(RecoveryAction::FallbackModel {
                    model: "claude-haiku".to_string(),
                    adjust_parameters: true,
                })
            }

            ClaudeError::NetworkTimeout => {
                Some(RecoveryAction::RefreshConnection {
                    create_new_session: false,
                    update_endpoint: false,
                })
            }

            _ => None,
        }
    }
}
```

### Circuit Breaker Implementation

The system includes a circuit breaker to prevent cascading failures:

```rust
#[derive(Debug)]
pub struct CircuitBreaker {
    state: Arc<Mutex<CircuitState>>,
    config: CircuitBreakerConfig,
    metrics: Arc<Mutex<CircuitMetrics>>,
}

#[derive(Debug, Clone)]
pub enum CircuitState {
    Closed,
    Open { opened_at: DateTime<Utc> },
    HalfOpen { test_request_count: u32 },
}

#[derive(Debug, Clone)]
pub struct CircuitBreakerConfig {
    pub failure_threshold: u32,
    pub success_threshold: u32,
    pub timeout: Duration,
    pub evaluation_window: Duration,
}

impl CircuitBreaker {
    pub async fn can_proceed(&self) -> bool {
        let state = self.state.lock().await;

        match &*state {
            CircuitState::Closed => true,
            CircuitState::Open { opened_at } => {
                Utc::now().signed_duration_since(*opened_at) > self.config.timeout
            }
            CircuitState::HalfOpen { test_request_count } => {
                *test_request_count < self.config.success_threshold
            }
        }
    }

    pub async fn record_success(&self) {
        let mut state = self.state.lock().await;
        let mut metrics = self.metrics.lock().await;

        metrics.consecutive_successes += 1;
        metrics.consecutive_failures = 0;

        match &*state {
            CircuitState::HalfOpen { test_request_count } => {
                if metrics.consecutive_successes >= self.config.success_threshold {
                    *state = CircuitState::Closed;
                    metrics.circuit_closed_at = Some(Utc::now());
                } else {
                    *state = CircuitState::HalfOpen {
                        test_request_count: test_request_count + 1,
                    };
                }
            }
            _ => {}
        }
    }

    pub async fn record_failure(&self) {
        let mut state = self.state.lock().await;
        let mut metrics = self.metrics.lock().await;

        metrics.consecutive_failures += 1;
        metrics.consecutive_successes = 0;

        match &*state {
            CircuitState::Closed => {
                if metrics.consecutive_failures >= self.config.failure_threshold {
                    *state = CircuitState::Open {
                        opened_at: Utc::now(),
                    };
                    metrics.circuit_opened_at = Some(Utc::now());
                }
            }
            CircuitState::HalfOpen { .. } => {
                *state = CircuitState::Open {
                    opened_at: Utc::now(),
                };
            }
            _ => {}
        }
    }
}
```

## Performance Monitoring

### Real-time Usage Analytics

The system provides comprehensive monitoring and analytics:

```rust
#[derive(Debug)]
pub struct ClaudeAnalytics {
    metrics_collector: Arc<MetricsCollector>,
    performance_tracker: Arc<PerformanceTracker>,
    cost_analyzer: Arc<CostAnalyzer>,
    dashboard: Arc<AnalyticsDashboard>,
}

#[derive(Debug, Clone, Serialize)]
pub struct PerformanceMetrics {
    pub requests_per_minute: f64,
    pub tokens_per_minute: f64,
    pub average_response_time: Duration,
    pub success_rate: f64,
    pub context_efficiency: f64,
    pub cost_per_task: f64,
    pub session_utilization: f64,
}

impl ClaudeAnalytics {
    pub async fn generate_performance_report(
        &self,
        time_window: Duration,
    ) -> Result<PerformanceReport> {
        let metrics = self.collect_metrics(time_window).await?;
        let trends = self.analyze_trends(&metrics).await?;
        let recommendations = self.generate_recommendations(&trends).await?;

        Ok(PerformanceReport {
            time_window,
            metrics,
            trends,
            recommendations,
            generated_at: Utc::now(),
        })
    }

    pub async fn real_time_dashboard(&self) -> Result<DashboardData> {
        let current_metrics = self.get_current_metrics().await?;
        let active_sessions = self.get_active_sessions_status().await?;
        let rate_limit_status = self.get_rate_limit_status().await?;
        let cost_tracking = self.get_cost_tracking().await?;

        Ok(DashboardData {
            current_metrics,
            active_sessions,
            rate_limit_status,
            cost_tracking,
            alerts: self.check_alerts().await?,
        })
    }
}
```

This comprehensive Claude Code integration provides robust, efficient, and cost-effective management of AI-assisted development workflows with sophisticated rate limiting, error recovery, and performance optimization.