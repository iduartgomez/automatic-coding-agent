# 1.5 Docker Deployment System

**Deliverable**: Container orchestration, volume management, and deployment strategies

## Overview

The Docker deployment system provides isolated, reproducible execution environments with sophisticated resource management, security controls, and operational monitoring. The system supports both local development and production deployment scenarios with comprehensive container lifecycle management.

## Container Architecture

### Multi-Stage Container Design

The system uses a multi-stage build approach for optimal image size and security:

```dockerfile
# Build stage
FROM rust:1.75-slim as builder

WORKDIR /build
RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Claude Code CLI
RUN curl -fsSL https://claude.ai/cli/install.sh | sh

# Copy and build the agent
COPY Cargo.toml Cargo.lock ./
COPY src/ ./src/
RUN cargo build --release --target x86_64-unknown-linux-musl

# Runtime stage
FROM alpine:3.19

RUN apk add --no-cache \
    ca-certificates \
    git \
    nodejs \
    npm \
    python3 \
    py3-pip \
    curl \
    jq \
    && addgroup -g 1000 agent \
    && adduser -u 1000 -G agent -s /bin/sh -D agent

# Copy Claude Code CLI
COPY --from=builder /root/.local/bin/claude /usr/local/bin/claude

# Copy agent binary
COPY --from=builder /build/target/x86_64-unknown-linux-musl/release/claude-code-agent /usr/local/bin/

# Create required directories
RUN mkdir -p /repos /workspace /session /logs /tmp/agent \
    && chown -R agent:agent /workspace /session /logs /tmp/agent

USER agent
WORKDIR /workspace

ENTRYPOINT ["/usr/local/bin/claude-code-agent"]
```

### Container Configuration Management

The system provides flexible container configuration with environment-specific overrides:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerConfig {
    pub image: String,
    pub tag: String,
    pub resources: ResourceLimits,
    pub volumes: Vec<VolumeMount>,
    pub environment: HashMap<String, String>,
    pub network: NetworkConfig,
    pub security: SecurityConfig,
    pub health_check: HealthCheckConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceLimits {
    pub memory_mb: u64,
    pub cpu_cores: f64,
    pub disk_mb: u64,
    pub ulimits: HashMap<String, u64>,
    pub cgroup_limits: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VolumeMount {
    pub host_path: PathBuf,
    pub container_path: PathBuf,
    pub mode: VolumeMode,
    pub volume_type: VolumeType,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VolumeMode {
    ReadOnly,
    ReadWrite,
    ReadWritePrivate,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum VolumeType {
    Bind,
    Volume,
    Tmpfs,
}
```

## Volume Management System

### Intelligent Volume Orchestration

The system manages complex volume mounting strategies for different use cases:

```rust
pub struct VolumeManager {
    docker_client: Arc<Docker>,
    volume_registry: Arc<Mutex<VolumeRegistry>>,
    cleanup_scheduler: Arc<CleanupScheduler>,
    security_validator: Arc<SecurityValidator>,
}

#[derive(Debug, Clone)]
pub struct VolumeRegistry {
    active_volumes: HashMap<VolumeId, VolumeInfo>,
    volume_pools: HashMap<PoolName, VolumePool>,
    mount_history: Vec<MountRecord>,
}

#[derive(Debug, Clone)]
pub struct VolumeInfo {
    pub id: VolumeId,
    pub name: String,
    pub path: PathBuf,
    pub size_mb: u64,
    pub created_at: DateTime<Utc>,
    pub last_accessed: DateTime<Utc>,
    pub mount_count: u32,
    pub volume_type: VolumeType,
    pub security_context: SecurityContext,
}

impl VolumeManager {
    pub async fn setup_session_volumes(
        &self,
        session_id: &SessionId,
        config: &VolumeConfiguration,
    ) -> Result<Vec<VolumeMount>> {
        let mut mounts = Vec::new();

        // Setup repository volumes (read-only)
        for repo_config in &config.repositories {
            let mount = self.setup_repository_volume(session_id, repo_config).await?;
            mounts.push(mount);
        }

        // Setup workspace volume (read-write)
        let workspace_mount = self.setup_workspace_volume(session_id, &config.workspace).await?;
        mounts.push(workspace_mount);

        // Setup session data volume (read-write, persistent)
        let session_mount = self.setup_session_volume(session_id).await?;
        mounts.push(session_mount);

        // Setup logs volume (write-only)
        let logs_mount = self.setup_logs_volume(session_id).await?;
        mounts.push(logs_mount);

        // Validate mount security
        self.validate_mount_security(&mounts).await?;

        // Register volumes for cleanup
        self.register_volumes_for_cleanup(session_id, &mounts).await?;

        Ok(mounts)
    }

    async fn setup_repository_volume(
        &self,
        session_id: &SessionId,
        repo_config: &RepositoryConfig,
    ) -> Result<VolumeMount> {
        // Create or reuse repository volume
        let volume_name = format!("repo-{}-{}", session_id, repo_config.name);

        let volume = match self.find_existing_repository_volume(&repo_config.url).await? {
            Some(existing) => {
                self.update_repository_volume(&existing, repo_config).await?
            }
            None => {
                self.create_repository_volume(session_id, repo_config).await?
            }
        };

        Ok(VolumeMount {
            host_path: volume.path,
            container_path: PathBuf::from(&format!("/repos/{}", repo_config.name)),
            mode: VolumeMode::ReadOnly,
            volume_type: VolumeType::Volume,
        })
    }

    async fn setup_workspace_volume(
        &self,
        session_id: &SessionId,
        config: &WorkspaceConfig,
    ) -> Result<VolumeMount> {
        let volume_name = format!("workspace-{}", session_id);

        // Create workspace volume with appropriate size
        let volume = self.docker_client
            .create_volume(CreateVolumeOptions {
                name: volume_name.clone(),
                driver: "local".to_string(),
                driver_opts: HashMap::from([
                    ("type".to_string(), "tmpfs".to_string()),
                    ("device".to_string(), "tmpfs".to_string()),
                    ("o".to_string(), format!("size={}m", config.size_mb)),
                ]),
                labels: HashMap::from([
                    ("session_id".to_string(), session_id.to_string()),
                    ("volume_type".to_string(), "workspace".to_string()),
                    ("created_at".to_string(), Utc::now().to_rfc3339()),
                ]),
            })
            .await?;

        Ok(VolumeMount {
            host_path: PathBuf::from(&volume.name),
            container_path: PathBuf::from("/workspace"),
            mode: VolumeMode::ReadWrite,
            volume_type: VolumeType::Volume,
        })
    }
}
```

### Volume Lifecycle Management

The system implements comprehensive volume lifecycle management:

```rust
#[derive(Debug)]
pub struct CleanupScheduler {
    cleanup_policies: Vec<CleanupPolicy>,
    scheduled_cleanups: Arc<Mutex<HashMap<VolumeId, CleanupSchedule>>>,
    cleanup_executor: Arc<CleanupExecutor>,
}

#[derive(Debug, Clone)]
pub struct CleanupPolicy {
    pub volume_types: Vec<VolumeType>,
    pub retention_period: Duration,
    pub cleanup_trigger: CleanupTrigger,
    pub cleanup_strategy: CleanupStrategy,
}

#[derive(Debug, Clone)]
pub enum CleanupTrigger {
    TimeBasedRetention,
    DiskSpaceThreshold { threshold_percent: f64 },
    SessionComplete,
    Manual,
}

#[derive(Debug, Clone)]
pub enum CleanupStrategy {
    ImmediateDelete,
    GracefulShutdown { grace_period: Duration },
    Archive { archive_location: PathBuf },
    Compress { compression_ratio: f64 },
}

impl CleanupScheduler {
    pub async fn schedule_volume_cleanup(
        &self,
        volume_id: VolumeId,
        policy: CleanupPolicy,
    ) -> Result<()> {
        let cleanup_time = match policy.cleanup_trigger {
            CleanupTrigger::TimeBasedRetention => {
                Utc::now() + policy.retention_period
            }
            CleanupTrigger::SessionComplete => {
                // Cleanup immediately after session ends
                Utc::now()
            }
            CleanupTrigger::DiskSpaceThreshold { .. } => {
                // Monitor-based cleanup, no fixed time
                return self.add_to_monitoring_queue(volume_id, policy).await;
            }
            CleanupTrigger::Manual => {
                // No automatic cleanup
                return Ok(());
            }
        };

        let schedule = CleanupSchedule {
            volume_id,
            scheduled_time: cleanup_time,
            policy,
            created_at: Utc::now(),
        };

        let mut scheduled = self.scheduled_cleanups.lock().await;
        scheduled.insert(volume_id, schedule);

        Ok(())
    }

    pub async fn execute_cleanup(&self, volume_id: VolumeId) -> Result<CleanupResult> {
        let schedule = {
            let mut scheduled = self.scheduled_cleanups.lock().await;
            scheduled.remove(&volume_id)
                .ok_or(CleanupError::ScheduleNotFound(volume_id))?
        };

        match schedule.policy.cleanup_strategy {
            CleanupStrategy::ImmediateDelete => {
                self.cleanup_executor.delete_volume_immediately(volume_id).await
            }

            CleanupStrategy::GracefulShutdown { grace_period } => {
                self.cleanup_executor
                    .shutdown_volume_gracefully(volume_id, grace_period)
                    .await
            }

            CleanupStrategy::Archive { archive_location } => {
                self.cleanup_executor
                    .archive_volume(volume_id, archive_location)
                    .await
            }

            CleanupStrategy::Compress { compression_ratio } => {
                self.cleanup_executor
                    .compress_volume(volume_id, compression_ratio)
                    .await
            }
        }
    }
}
```

## Container Orchestration

### Advanced Container Lifecycle Management

The system provides sophisticated container orchestration with health monitoring:

```rust
pub struct ContainerOrchestrator {
    docker_client: Arc<Docker>,
    container_registry: Arc<Mutex<ContainerRegistry>>,
    health_monitor: Arc<HealthMonitor>,
    resource_monitor: Arc<ResourceMonitor>,
    networking_manager: Arc<NetworkingManager>,
}

#[derive(Debug, Clone)]
pub struct ContainerRegistry {
    active_containers: HashMap<ContainerId, ContainerInfo>,
    session_mappings: HashMap<SessionId, ContainerId>,
    resource_allocations: HashMap<ContainerId, ResourceAllocation>,
}

impl ContainerOrchestrator {
    pub async fn launch_session_container(
        &self,
        session_id: SessionId,
        config: ContainerConfig,
        volumes: Vec<VolumeMount>,
    ) -> Result<ContainerId> {
        // Prepare container configuration
        let container_config = self.build_container_config(session_id.clone(), config, volumes).await?;

        // Create and start container
        let container_id = self.create_container(container_config).await?;

        // Start health monitoring
        self.health_monitor.start_monitoring(container_id.clone()).await?;

        // Start resource monitoring
        self.resource_monitor.start_monitoring(container_id.clone()).await?;

        // Register container
        self.register_container(session_id, container_id.clone()).await?;

        Ok(container_id)
    }

    async fn build_container_config(
        &self,
        session_id: SessionId,
        config: ContainerConfig,
        volumes: Vec<VolumeMount>,
    ) -> Result<CreateContainerOptions<String>> {
        let mut env_vars = config.environment.clone();
        env_vars.insert("SESSION_ID".to_string(), session_id.to_string());
        env_vars.insert("RUST_LOG".to_string(), "info".to_string());

        let mut host_config = HostConfig {
            memory: Some(config.resources.memory_mb * 1024 * 1024),
            nano_cpus: Some((config.resources.cpu_cores * 1_000_000_000.0) as i64),
            binds: Some(self.convert_volume_mounts_to_binds(&volumes)),
            ..Default::default()
        };

        // Add security configurations
        if config.security.read_only_root_filesystem {
            host_config.read_only_root_fs = Some(true);
        }

        if config.security.no_new_privileges {
            host_config.security_opt = Some(vec!["no-new-privileges:true".to_string()]);
        }

        let container_config = Config {
            image: Some(format!("{}:{}", config.image, config.tag)),
            env: Some(env_vars.into_iter()
                .map(|(k, v)| format!("{}={}", k, v))
                .collect()),
            host_config: Some(host_config),
            healthcheck: Some(self.build_health_check(&config.health_check)?),
            labels: Some(HashMap::from([
                ("session_id".to_string(), session_id.to_string()),
                ("created_by".to_string(), "claude-code-agent".to_string()),
                ("created_at".to_string(), Utc::now().to_rfc3339()),
            ])),
            ..Default::default()
        };

        Ok(CreateContainerOptions {
            name: format!("claude-agent-{}", session_id),
            platform: Some("linux/amd64".to_string()),
        })
    }
}
```

### Resource Monitoring and Management

The system implements comprehensive resource monitoring:

```rust
#[derive(Debug)]
pub struct ResourceMonitor {
    docker_client: Arc<Docker>,
    monitored_containers: Arc<Mutex<HashMap<ContainerId, MonitoringState>>>,
    metrics_collector: Arc<MetricsCollector>,
    alert_manager: Arc<AlertManager>,
}

#[derive(Debug, Clone)]
pub struct ResourceUsage {
    pub cpu_percent: f64,
    pub memory_usage_mb: u64,
    pub memory_limit_mb: u64,
    pub disk_usage_mb: u64,
    pub network_rx_bytes: u64,
    pub network_tx_bytes: u64,
    pub timestamp: DateTime<Utc>,
}

impl ResourceMonitor {
    pub async fn start_monitoring(&self, container_id: ContainerId) -> Result<()> {
        let monitoring_state = MonitoringState::new(container_id.clone());

        {
            let mut monitored = self.monitored_containers.lock().await;
            monitored.insert(container_id.clone(), monitoring_state);
        }

        // Start monitoring loop
        let monitor = Arc::clone(self);
        let container_id_clone = container_id.clone();

        tokio::spawn(async move {
            monitor.monitoring_loop(container_id_clone).await;
        });

        Ok(())
    }

    async fn monitoring_loop(&self, container_id: ContainerId) {
        let mut interval = tokio::time::interval(Duration::from_secs(10));

        loop {
            interval.tick().await;

            match self.collect_resource_metrics(&container_id).await {
                Ok(metrics) => {
                    // Store metrics
                    self.metrics_collector.record_metrics(&container_id, &metrics).await;

                    // Check for resource alerts
                    if let Err(e) = self.check_resource_alerts(&container_id, &metrics).await {
                        eprintln!("Failed to check resource alerts: {}", e);
                    }
                }
                Err(e) => {
                    eprintln!("Failed to collect metrics for container {}: {}", container_id, e);

                    // Check if container still exists
                    if !self.container_exists(&container_id).await {
                        break;
                    }
                }
            }
        }

        // Cleanup monitoring state
        let mut monitored = self.monitored_containers.lock().await;
        monitored.remove(&container_id);
    }

    async fn collect_resource_metrics(&self, container_id: &ContainerId) -> Result<ResourceUsage> {
        let stats = self.docker_client
            .stats(container_id, Some(StatsOptions {
                stream: false,
                one_shot: true,
            }))
            .next()
            .await
            .ok_or(ResourceError::StatsUnavailable)?
            .map_err(ResourceError::DockerError)?;

        // Calculate CPU percentage
        let cpu_percent = if let (Some(cpu_stats), Some(precpu_stats)) =
            (&stats.cpu_stats, &stats.precpu_stats) {

            let cpu_delta = cpu_stats.cpu_usage.total_usage - precpu_stats.cpu_usage.total_usage;
            let system_delta = cpu_stats.system_cpu_usage.unwrap_or(0) -
                precpu_stats.system_cpu_usage.unwrap_or(0);

            if system_delta > 0 && cpu_delta > 0 {
                let num_cpus = cpu_stats.cpu_usage.percpu_usage.as_ref()
                    .map(|v| v.len())
                    .unwrap_or(1);

                (cpu_delta as f64 / system_delta as f64) * num_cpus as f64 * 100.0
            } else {
                0.0
            }
        } else {
            0.0
        };

        // Extract memory usage
        let memory_usage_mb = stats.memory_stats.usage.unwrap_or(0) / 1024 / 1024;
        let memory_limit_mb = stats.memory_stats.limit.unwrap_or(0) / 1024 / 1024;

        // Extract network usage
        let (network_rx_bytes, network_tx_bytes) = if let Some(networks) = &stats.networks {
            let mut rx_total = 0;
            let mut tx_total = 0;

            for network in networks.values() {
                rx_total += network.rx_bytes;
                tx_total += network.tx_bytes;
            }

            (rx_total, tx_total)
        } else {
            (0, 0)
        };

        // Calculate disk usage (approximate from container inspect)
        let disk_usage_mb = self.estimate_disk_usage(container_id).await.unwrap_or(0);

        Ok(ResourceUsage {
            cpu_percent,
            memory_usage_mb,
            memory_limit_mb,
            disk_usage_mb,
            network_rx_bytes,
            network_tx_bytes,
            timestamp: Utc::now(),
        })
    }

    async fn check_resource_alerts(
        &self,
        container_id: &ContainerId,
        metrics: &ResourceUsage,
    ) -> Result<()> {
        // Check memory usage
        if metrics.memory_limit_mb > 0 {
            let memory_percent = (metrics.memory_usage_mb as f64 / metrics.memory_limit_mb as f64) * 100.0;

            if memory_percent > 90.0 {
                self.alert_manager.send_alert(ResourceAlert {
                    container_id: container_id.clone(),
                    alert_type: AlertType::HighMemoryUsage,
                    severity: AlertSeverity::Critical,
                    value: memory_percent,
                    threshold: 90.0,
                    timestamp: metrics.timestamp,
                }).await?;
            }
        }

        // Check CPU usage
        if metrics.cpu_percent > 95.0 {
            self.alert_manager.send_alert(ResourceAlert {
                container_id: container_id.clone(),
                alert_type: AlertType::HighCpuUsage,
                severity: AlertSeverity::Warning,
                value: metrics.cpu_percent,
                threshold: 95.0,
                timestamp: metrics.timestamp,
            }).await?;
        }

        Ok(())
    }
}
```

## Network Configuration

### Secure Networking Setup

The system implements secure network configuration with isolation:

```rust
#[derive(Debug)]
pub struct NetworkingManager {
    docker_client: Arc<Docker>,
    network_registry: Arc<Mutex<NetworkRegistry>>,
    security_policies: Vec<NetworkSecurityPolicy>,
}

#[derive(Debug, Clone)]
pub struct NetworkConfig {
    pub network_mode: NetworkMode,
    pub port_mappings: Vec<PortMapping>,
    pub dns_config: DnsConfig,
    pub security_policies: Vec<String>,
    pub bandwidth_limits: Option<BandwidthLimits>,
}

#[derive(Debug, Clone)]
pub enum NetworkMode {
    Bridge,
    Host,
    None,
    Container(String),
    Custom(String),
}

impl NetworkingManager {
    pub async fn setup_container_network(
        &self,
        session_id: &SessionId,
        config: &NetworkConfig,
    ) -> Result<NetworkSetup> {
        let network_name = format!("claude-agent-{}", session_id);

        // Create isolated network if using custom mode
        let network_id = match &config.network_mode {
            NetworkMode::Custom(_) | NetworkMode::Bridge => {
                self.create_isolated_network(&network_name, session_id).await?
            }
            _ => None,
        };

        // Apply security policies
        self.apply_network_security_policies(&network_name, &config.security_policies).await?;

        Ok(NetworkSetup {
            network_id,
            network_name,
            isolation_level: self.calculate_isolation_level(config),
        })
    }

    async fn create_isolated_network(
        &self,
        network_name: &str,
        session_id: &SessionId,
    ) -> Result<Option<String>> {
        let network_config = CreateNetworkOptions {
            name: network_name.to_string(),
            driver: "bridge".to_string(),
            options: HashMap::from([
                ("com.docker.network.bridge.enable_icc".to_string(), "false".to_string()),
                ("com.docker.network.bridge.enable_ip_masquerade".to_string(), "true".to_string()),
            ]),
            labels: HashMap::from([
                ("session_id".to_string(), session_id.to_string()),
                ("created_by".to_string(), "claude-code-agent".to_string()),
            ]),
            ..Default::default()
        };

        let network = self.docker_client
            .create_network(network_config)
            .await?;

        Ok(Some(network.id))
    }
}
```

## Security Implementation

### Comprehensive Container Security

The system implements multi-layered security controls:

```rust
#[derive(Debug, Clone)]
pub struct SecurityConfig {
    pub read_only_root_filesystem: bool,
    pub no_new_privileges: bool,
    pub user_namespace: Option<String>,
    pub seccomp_profile: Option<String>,
    pub apparmor_profile: Option<String>,
    pub capabilities_drop: Vec<String>,
    pub capabilities_add: Vec<String>,
    pub security_opt: Vec<String>,
}

#[derive(Debug)]
pub struct SecurityValidator {
    policies: Vec<SecurityPolicy>,
    audit_logger: Arc<AuditLogger>,
    compliance_checker: Arc<ComplianceChecker>,
}

impl SecurityValidator {
    pub async fn validate_container_security(
        &self,
        config: &ContainerConfig,
    ) -> Result<SecurityValidationResult> {
        let mut violations = Vec::new();
        let mut recommendations = Vec::new();

        // Check for read-only root filesystem
        if !config.security.read_only_root_filesystem {
            violations.push(SecurityViolation {
                severity: ViolationSeverity::Medium,
                description: "Root filesystem should be read-only".to_string(),
                remediation: "Set read_only_root_filesystem: true".to_string(),
            });
        }

        // Check privilege escalation prevention
        if !config.security.no_new_privileges {
            violations.push(SecurityViolation {
                severity: ViolationSeverity::High,
                description: "Container allows privilege escalation".to_string(),
                remediation: "Set no_new_privileges: true".to_string(),
            });
        }

        // Check for excessive capabilities
        let dangerous_caps = self.check_dangerous_capabilities(&config.security.capabilities_add);
        for cap in dangerous_caps {
            violations.push(SecurityViolation {
                severity: ViolationSeverity::High,
                description: format!("Dangerous capability {} granted", cap),
                remediation: format!("Remove {} from capabilities_add", cap),
            });
        }

        // Generate security recommendations
        recommendations.extend(self.generate_security_recommendations(config).await?);

        Ok(SecurityValidationResult {
            is_secure: violations.iter().all(|v| v.severity != ViolationSeverity::Critical),
            violations,
            recommendations,
            security_score: self.calculate_security_score(&violations),
        })
    }

    fn check_dangerous_capabilities(&self, capabilities: &[String]) -> Vec<String> {
        let dangerous = [
            "SYS_ADMIN", "SYS_MODULE", "SYS_RAWIO", "SYS_PTRACE",
            "DAC_OVERRIDE", "DAC_READ_SEARCH", "SYS_TIME",
        ];

        capabilities
            .iter()
            .filter(|cap| dangerous.contains(&cap.as_str()))
            .cloned()
            .collect()
    }
}
```

## Deployment Strategies

### Multi-Environment Deployment

The system supports various deployment scenarios:

```rust
#[derive(Debug, Clone)]
pub enum DeploymentStrategy {
    LocalDevelopment {
        host_network: bool,
        debug_mode: bool,
        volume_persistence: bool,
    },
    SingleNode {
        resource_isolation: bool,
        security_hardening: bool,
        monitoring_enabled: bool,
    },
    Distributed {
        node_count: u32,
        load_balancing: LoadBalancingStrategy,
        failure_recovery: FailureRecoveryStrategy,
    },
    Cloud {
        provider: CloudProvider,
        auto_scaling: AutoScalingConfig,
        backup_strategy: BackupStrategy,
    },
}

pub struct DeploymentManager {
    docker_client: Arc<Docker>,
    deployment_history: Arc<Mutex<Vec<DeploymentRecord>>>,
    rollback_manager: Arc<RollbackManager>,
}

impl DeploymentManager {
    pub async fn deploy_with_strategy(
        &self,
        strategy: DeploymentStrategy,
        config: ContainerConfig,
    ) -> Result<DeploymentResult> {
        match strategy {
            DeploymentStrategy::LocalDevelopment { debug_mode, .. } => {
                self.deploy_local_development(config, debug_mode).await
            }

            DeploymentStrategy::SingleNode { security_hardening, .. } => {
                self.deploy_single_node(config, security_hardening).await
            }

            DeploymentStrategy::Distributed { node_count, .. } => {
                self.deploy_distributed(config, node_count).await
            }

            DeploymentStrategy::Cloud { provider, .. } => {
                self.deploy_cloud(config, provider).await
            }
        }
    }

    async fn deploy_local_development(
        &self,
        mut config: ContainerConfig,
        debug_mode: bool,
    ) -> Result<DeploymentResult> {
        if debug_mode {
            // Add debug configurations
            config.environment.insert("RUST_LOG".to_string(), "debug".to_string());
            config.environment.insert("CLAUDE_DEBUG".to_string(), "true".to_string());

            // Mount source code for hot reloading (if available)
            config.volumes.push(VolumeMount {
                host_path: PathBuf::from("./src"),
                container_path: PathBuf::from("/app/src"),
                mode: VolumeMode::ReadOnly,
                volume_type: VolumeType::Bind,
            });
        }

        let container_id = self.launch_container(config).await?;

        Ok(DeploymentResult {
            deployment_id: format!("local-{}", Utc::now().timestamp()),
            container_ids: vec![container_id],
            network_endpoints: vec![],
            status: DeploymentStatus::Running,
        })
    }
}
```

This comprehensive Docker deployment system provides secure, scalable, and maintainable container orchestration for the Claude Code Agent with sophisticated resource management, security controls, and operational monitoring capabilities.